# EvalAgent

We propose **EvalAgent**, aimed at extracting evaluation criteria from instructional web documents. Our framework comprises several key components. At a high level, given a user promopt, we first generate search queries that can be easily answered using instructional web documents. After retrieving such documents, we generate answers for the search queries. We then combine these answers to generate evaluation criteria that is grounded in instructional documents.

<img src="images/Eval_Agents_Overview.png" width="90%" height="75%">

The figure above gives an overview of our framework. 
1. Step 1: Query Generation 
2. Step 2: Expert Web Retriever 
3. Step 3: Criteria Generator 

## üõ†Ô∏è Installation 
Python environment set up:

`
pip install -r requirements.txt
`

After setting the environment, we need to set a few keys/environment variables to do the following:
1. Model API keys you want to use for running query generation, answering and criteria aggregation 
2. Search set up - for retrieving URLs

Our search is backed by Google Search API. The set up instructions can be found [here](https://github.com/ManyaWadhwa/EvalAgent/blob/main/google_setup.md).

[optional set up] 
Reddit URLs are often retrieved by our search queries, so we use `praw` to scrap we also set up reddit scraping. You can find the setup instructions [here](https://github.com/ManyaWadhwa/EvalAgent/blob/main/reddit_setup.md). This setup is optional, if the environment variables are not setup you won't get an error, we just won't include that URL.

### Environment variables
Once you have gone through the set up above make sure you have the following variables populated in `environment_variables.sh`
```
## OpenAI if you using their models; else set up anthropic keys if needed
export OPENAI_API_KEY=
## Google search credentials 
export GOOGLE_SEARCH_API_KEY=
export CSE_ID=
## OPTIONAL reddit credentials
export reddit_client_id=
export reddit_client_secret=
export reddit_user_agent=
export reddit_username=
export reddit_password=
```

## Running EvalAgent

Our proposed method does the following: (1) generates criteria based on instruction web-documents (EA-Web) (2) combines EA-Web with LLM-prompted criteria (EA-Full)

To generate the proposed criteria, set the config file as following:
```
[criteria_gen_args.yaml]
input_file: "data/sample.jsonl"
output_file: "data/sample_criteria.jsonl"
ea: true
llm: true
search: true
score: true
query_model: gpt-4o-mini-2024-07-18
aggregator_model: gpt-4o-mini-2024-07-18
answer_model: gpt-4o-mini-2024-07-18
scoring_model: gpt-4o-mini-2024-07-18
n: 10 
```

Then set the environment variables and run the criteria generation:
```
./environment_variables.sh
python evaluation_criteria_generator.py --config criteria_gen_args.yaml
```

`evaluation_criteria_generator.py` outputs a JSON file and in the above configuration it will have three types of criteria:
1. llm_criteria : LLM prompted criteria that generates _n_ criteria 
2. ea_criteria: EA-Web criteria generated from instructional web documents 
3. ea_full_criteria: merged criteria that combines LLM and EA-Web 

## Evaluating with EvalAgent 

```
```

## Alternate modes for criteria gen with EvalAgent

Our framework provides users the ability to generate criteria in various modes:
1. LLM-n: LLM only generated criteria up to _n_ points 
2. EvalAgent-LLM: EvalAgent pipeline where the criteria is generated by aggregating LLM-answered queries (compared to retrieving web-documents)
3. EvalAgent-Web: this mode generates criteria by searching the web for queries and combining instructional information from these web documents

To run only LLM-_n_ use the following config:
```
[criteria_gen_args.yaml]
input_file: "data/sample.jsonl"
output_file: "data/sample_criteria.jsonl"
ea: false
llm: true
search: false
score: false
query_model: gpt-4o-mini-2024-07-18
aggregator_model: gpt-4o-mini-2024-07-18
answer_model: gpt-4o-mini-2024-07-18
scoring_model: gpt-4o-mini-2024-07-18
n: 10 
```

To run only EvalAgent-LLM use the following config:
```
[criteria_gen_args.yaml]
input_file: "data/sample.jsonl"
output_file: "data/sample_criteria.jsonl"
ea: true
llm: false
search: false
score: false
query_model: gpt-4o-mini-2024-07-18
aggregator_model: gpt-4o-mini-2024-07-18
answer_model: gpt-4o-mini-2024-07-18
scoring_model: gpt-4o-mini-2024-07-18
n: 10 
```

To run only EvalAgent-Web use the following config:
```
[criteria_gen_args.yaml]
input_file: "data/sample.jsonl"
output_file: "data/sample_criteria.jsonl"
ea: true
llm: false
search: true
score: false
query_model: gpt-4o-mini-2024-07-18
aggregator_model: gpt-4o-mini-2024-07-18
answer_model: gpt-4o-mini-2024-07-18
scoring_model: gpt-4o-mini-2024-07-18
n: 10 
```

## Visualization 

We also have a flask-based UI where you can load the jsonl output from the search-based criteria generation. You can load the UI with the following command:
```
cd data/
python app.py --data ../data/sample_data_criteria_search.jsonl
```